# Basic Benchmark Evaluation Configuration
# This configuration runs evaluation on a subset of datasets with standard settings
# 
# IMPORTANT: Replace the model paths below with your actual model paths

# Model Configuration
model:
  path: "/path/to/your/model"  # CHANGE THIS: Path to your model
  backend: "vllm"  # Options: "vllm", "huggingface"
  use_vllm: true   # Whether to use vLLM acceleration
  
  # vLLM specific settings
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.9
  max_model_len: 32768
  trust_remote_code: true
  
  # Generation settings
  temperature: 0.7
  top_p: 0.9
  max_tokens: 2048
  system_prompt: "You are a helpful assistant that solves mathematical problems step by step."

# Dataset Configuration
datasets:
  data_dir: "../data"  # Path to benchmark datasets
  datasets:
    - "math"
    - "gsm8k"
    - "aime"
  split: "test"
  max_samples: 100  # Limit samples for quick testing

# Evaluator Configuration
evaluator:
  type: "xverify"
  model_path: "/path/to/xverify/model"  # CHANGE THIS: Path to xVerify model (e.g., IAAR-Shanghai/xVerify-0.5B-I)
  model_name: "xVerify-0.5B-I"
  use_vllm: true
  process_num: 5
  temperature: 0.1
  max_tokens: 2048
  gpu_memory_utilization: 0.8  # Main model is released before xVerify initialization

# Output Configuration
output:
  results_dir: "./results"
  save_individual_results: true
  save_summary: true

# Execution Configuration
execution:
  batch_size: 16
  eval_batch_size: 32
  num_workers: 4
  seed: 42
